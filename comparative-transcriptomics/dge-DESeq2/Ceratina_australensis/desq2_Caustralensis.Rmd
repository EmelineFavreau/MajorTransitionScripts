---
title: "DESeq2 Analyses: <i>Ceratina australensis</i>"
author: "KS Geist"
date: "8 February 2021"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = F, warning = F, include = F}
#install.packages("htmltools")
#BiocManager::install("DESeq2")
## Install BiocManager if you don't already have it first.

library(DESeq2)
library(htmltools)
library(vsn)
library(ggplot2)
library(tidyverse)
library(pheatmap)
```

## Getting Started

#### Raw Count Data

First, read in the raw count data:
```{r}
## Clear workspace
rm(list=ls())

## Import data
hymn <- read.table("Caustralensis_merged_gene_counts.txt", header = T, sep = "\t")

## Need to remove an unecessary second column & save the gene IDs as a vector for the row names:
geneID <- hymn[,1]
hymn <- hymn[,-c(1:2)]

## Rename the headers to the sample names:
# names(hymn)
colnames(hymn) <- c("SRR2916631", "SRR2917152", "SRR2916652", "SRR2915407", "SRR2916026", "SRR2916025", "SRR2916647", "SRR2916699", "SRR2917198", "SRR2916637", "SRR2916648", "SRR2916653")
dim(hymn)

## Next, let's reorder them by phenotype, R then NR:
hymn <- hymn[,c("SRR2915407", "SRR2916026", "SRR2916025","SRR2916648", "SRR2916653", "SRR2916652",        "SRR2916631","SRR2916637","SRR2916647", "SRR2916699", "SRR2917198","SRR2917152")]
#names(hymn)
## Right now, I'm mirroring the reproductive vs. non comparison performed in the original paper. Verify this is what we want.

## Next, move the gene IDs to the row names instead of as a column:
rownames(hymn) <- geneID

## Check that the df looks as expected:
head(hymn)

```

I've noticed that very few of the genes seem to have zero counts. 
```{r}
## How many total genes:
nrow(hymn)

## One quick quality check: how many of my gene counts are zero for all samples?
totalCounts <- rowSums(hymn)
length(totalCounts[totalCounts == 0])
## Proportion of all genes not expressed AT ALL:
length(totalCounts[totalCounts == 0]) / length(totalCounts)

## How about if we have 0 counts for ANY of the samples?
anyZero <- colSums(hymn == 0)
anyZero ## the number of zeros seen in each column
## Now, let's get a proportion of any gene in any sample that was 0:
sum(anyZero) / sum(hymn)

## Read count by phenotype (first 6 are R's, second 6 are NR's)
sum(hymn[,c(1:6)])
sum(hymn[,c(7:12)])

# Total read count:
sum(hymn)
```
I don't know if this is important, but it's noteworthy that -- unnormalized -- we see a very low level of zero expression. Normalization will not affect the number of *true* zeros, but this is precisefly why we look at log-fold change instead.

**Note that 36% of reads have 0 counts across all samples. Total read counts: 20702772**

#### Create the phenotype metadata
Create the pheno metadata, making sure that the row names of the metadata dataframe are present and in the same order as the column names of the counts dataframe:
```{r}
#id <- c("SRR2915407", "SRR2916026", "SRR2916025","SRR2916631","SRR2916637","SRR2916647") primary vs. secondary
id <- c("SRR2915407", "SRR2916026", "SRR2916025","SRR2916648", "SRR2916653", "SRR2916652",        "SRR2916631","SRR2916637","SRR2916647", "SRR2916699", "SRR2917198","SRR2917152")

phenotype <- c(rep("R",6), rep("NR", 6))

pheno <- as.data.frame(phenotype)
row.names(pheno) <- id
#pheno

colnames(hymn)

# Notice that the DF is analogous to the expression data

# Check that the column names of the counts df are the same and in the same order as the phenotypes df:
all(colnames(hymn) %in% rownames(pheno))
all(colnames(hymn) == rownames(pheno))
```

## Run DESeq2

Construct DESEQDataSet Object (stored as dds):
```{r}
dds <- DESeqDataSetFromMatrix(countData=hymn, 
                              colData=pheno, 
                              design=~phenotype)
## Converts counts to integer mode??

## Design specifies how the counts from each gene depend on our variables in the metadata; we currently only have one factor of interest, phenotype

## tidy=TRUE argument tells DESeq2 to output the results table with rownames as a first column called 'row; alternatively, I could have used that argument here to move the first col into the rownames of each dataset.

## Check the structure of the object:
#str(dds)

## Extract information from the object, if desired:
#counts(dds) ## extracts back out the original counts matrix
```

#### Run DESeq Function
* **Performs normalization (median of ratios):**
  1.Corrects for variance in read sequencing depth
  2.Corrects for inter-library dispersion in counts (for each gene)

* **Calculates the significance of coefficients with a negative binomial GLM**

Note that DESeq2 does not actually use the normalized counts but rather uses the raw counts and models the normalization in the negative binomial model.   
```{r}
## Yes, we are overwriting / performing the functions on the dataset object
dds <- DESeq(dds)
## Note that normalization only could be performed as follows:
# dds <- estimateSizeFactors(dds)
# sizeFactors(dds)

## Let's extract the normalized counts (from either the DESeq() function or the estimateSizeFactors() function, whichever was used above):
normalized_counts <- counts(dds, normalized=TRUE)
#View(normalized_counts)
write.table(normalized_counts, file="normalized_counts.txt", sep="\t", quote=F, col.names=NA)

## I would also like to write the results file for future use / reference:
ddsResults <- results(dds)
write.table(ddsResults, file="DESeq2_results.txt", sep="\t", quote=F, col.names=NA)
```

## More Quality Control & Visualizing Gene & Sample-Wise Variation

#### Estimating Gene-Wise Dispersion
DESeq2 uses a specific measure of dispersion ($\alpha$) related to the mean ($\mu$) and variance of the data: $\sigma^2 = \mu + \alpha * \mu^2$. This means that genes with moderate to high counts, the square root of dispersion ($\alpha$) equals the Coefficient of Variation ($CV = \sigma^2 / \mu$). Thus, a 0.01 dispersion means there is 10% variation around the mean expected across biological replicates.

```{r}
meanCounts <- rowMeans(hymn)
varCounts <- apply(hymn, 1, var)  ## Apply the variance function by margin = 1, which is rows
plot(log(varCounts)~log(meanCounts), ylab = "log Variance", xlab = "log Mean", main = "\nLog-log plot of variance by mean for each gene\n should be linear.\n", pch = 16, cex = 0.75)
abline(lm(log(varCounts+0.0001)~log((meanCounts+0.0001))), col = "blue", lty = 2, lwd = 1.5)


```
The relationship between mean and variance should be linear on the log scale, and in gene expression data we predict that for higher means, we can more accurately predict the variance (it should be fanned out at lower means and a tighter relationship at higher means). We expect that for low mean counts, the variance estimates have a much larger spread, such that the dispersion estimates will differ much more between genes with small means.

**This is what we are seeing here.** A variance-stabilizing transformation will help to resolve this issue.

All of the variance stabilizing transformation (VST) functions provided by DESeq2 attempt to shrink the gene-wise dispersion. **Note: there are compelling arguments that these VSTs do a better job than a mixed model would, which is why we do not / cannot add random effects to our NB model.**

When we perform the VST & visualize it, we can ensure that our data are a good fit for DESeq2. In Visualization, the VST will allow us to more accurately identify DE genes. The shrinkage is being performed in the NB model as part of the DESeq2 function, which is important to reduce false positives. 

What does the plot of our dispersion estimates look like after model-fitting? 
```{r}
plotDispEsts(dds)
```
It looks good. The original data (black) already fit pretty well, and post-shrinkage (blue) everything is fitting nicely. There is no pattern to the data nor a huge cloud of points.

#### Variance Transformations
Transformations: Variance Stabilizing Functions - Parametric Fit
### Note that all transformations have set blind = FALSE. We do not want the transformations to be blind to the experimental design (phenotypes) at this point; we expect large differential expression in some genes, and so we need to control for outliers by setting blind to FALSE.
```{r}
#vsdata <- vst(dds, blind=FALSE) 
## This is a quicker function but not as thorough
vsdata1 <- varianceStabilizingTransformation(dds, blind = FALSE, fitType = "parametric")
meanSdPlot(assay(vsdata1))
```

Transformations: Variance Stabilizing Functions - Local Fit
```{r}
vsdata2 <- varianceStabilizingTransformation(dds, blind = FALSE, fitType = "local")
meanSdPlot(assay(vsdata2))
```

Transformations: Variance Stabilizing Functions - Mean Type
```{r}
vsdata3 <- varianceStabilizingTransformation(dds, blind = FALSE, fitType = "mean")
meanSdPlot(assay(vsdata3))
```

Transformations: Regularized Log, recommended for smaller numbers of samples
```{r}
rld <- rlog(dds, blind = FALSE)
meanSdPlot(assay(rld))
```
Heuristically, they all appear to do equally well except the rld(??) even though it is recommended for small sample sizes. Let's just stick with the parametric VST.

```{r}
#save(vsdata1, file = "Caustralensis_VarStab.RData")
write.csv(file = "Caustralensis_VarStab.csv", assay(vsdata1), row.names = T, quote = F)
```

#### Principal Component Analysis
```{r}
plotPCA(vsdata1, intgroup="phenotype", ntop = 300)
## I have changed ntop from the default of 500 to 300 because we're not anticipating huge numbers of DE genes. ntop plots the top XX most variable genes in the dataset.
```
Some Rs and non are quite dissimilar, and they mostly cluster.

**Hierarchical Clustering**
There is no built-in function for heatmaps in DESeq2 so let's use the pheatmap() function from the pheatmap package. 
```{r}
## It requires a matrix/dataframe of numeric values as input, which we can extract with the assay() function from our VST object.
vsdata1.assay <- assay(vsdata1)

## Compute  pairwise correlation values for samples:
pwCorr <- cor(vsdata1.assay)
#pwCorr   ## check the output, make note of the rownames and colnames

## Lastly, let's make our heatmap!
pheatmap(pwCorr)
```
There is similar clustering as observed in the PCA between R and NR phenotypes.

#### Calculating Log-fold Change & Getting DE Genes
In the original DESeq2 paper, the authors shrink the log-fold change estimates toward zero for genes with low counts and/or high dispersion values. It uses the distribution of LFC estimates as a prior to perform this shrinkage. In the most recent versions of DESeq2, the shrinkage of LFC estimates is not performed by default.

```{r}
## These are the LFC estimates without shrinkage:
#LFC <- ddsResults$log2FoldChange

## These are the LFC results with shrinkage:
#shrunkenLFC <- lfcShrink(dds = dds, coef = 1)
#shrunkenLFC
```
*However, note that these shrunked LFC values will not change the number of DE genes. The reason to do this is for additional downstream assessments. For example, if you wanted to subset your sig genes based on foldchange for further evaluation, you would want to use these values instead.*

##### Numbers of DE Genes Based on Fold-Change (Wald Tests)
Since we want a standardized way to retrieve DE genes for all our species, rather than using the commonly used Tibble filtering people will do, we will do hypothesis tests using set significance and log-fold change thresholds. This is a more conservative approach, so you will receive fewer genes than with the Tibble filtering method.

Quick breakdown of LFC:
```{r}
linear.changes <- c(1.5, 2, 2.5, 3, 4)
lfc.thresholds <- log(linear.changes, base = 2)
lfc.thresholds
## Thus, a LFC of 0.58 is equivalent to a linear fold change of 1.5 in the gene expression value.
```

```{r}
## This performs a Wald test of the specified contrast with a set significance threshold and a LFC threshold
## Can use a theshold of 0.05 since they use a BH adjustment to the p-value
foldchange <- results(dds, alpha = 0.05, lfcThreshold = 0.58)  
# head(foldchange, tidy = T)
# summary(foldchange)

## Function to calculate the number of DE genes for each of the LFC values:
LFC.df <- NULL
DEGenumerator <- function(dds) {
  for (i in 1:length(lfc.thresholds)) {
    ## Do the Wald tests with each of the LFC thresholds you want to try
    foldchange <- results(dds, alpha = 0.05, lfcThreshold = lfc.thresholds[i])  
    ## Get the total number of DE genes
    total <- nrow(subset(foldchange, padj < 0.05))
    ## Get the up genes, greater than the positive LFC threshold
    up <- nrow(subset(foldchange, padj < 0.05 & log2FoldChange > lfc.thresholds[i]))
    ## Get the down genes, less than the negative LFC threshold
    down <- nrow(subset(foldchange, padj < 0.05 & log2FoldChange < -lfc.thresholds[i]))
    ## Put in a dataframe:
    LFC.df <- rbind(LFC.df, c(lfc.thresholds[i], linear.changes[i], total, up, down, "Caustralensis"))
  }
  return(as.data.frame(LFC.df))
}

LFC.df <- DEGenumerator(dds)
names(LFC.df) <- c("LFC.threshold", "Linear.Change", "Total.DEGs", "Up.DEGs", "Down.DEGs", "Species")
LFC.df
## Writes the number of DEGs to an appended file in the DEG main directory
#write.table(LFC.df, file = "../DEG_counts.txt", append = T, quote = F, row.names = F, col.names = T)
```

#### The *post hoc* Filtering Method
Alternatively, we can externally set the p-cutoff and log-fold change cutoff and filter the results from there. Note that the lfc.cutoff is set to 0.58, which translates to an actual fold change of 1.5.

```{r, include = TRUE}
## Set the contrast of interest (R vs. NR). Contrasts are of the form ("sample_type", "contrast2compare", "base_condition")
contrastRNR <- c("phenotype", "R", "NR")
## Note: if I don't set my contrast, it will default to taking the lowest category alphabetically as the base  condition

## To filter the ddsResults based on an FDR of 0.05 (versus 0.1, which is the default) we also need to set the alpha parameter to 0.05 before running the results() function:
#ddsResults <- results(dds, alpha = 0.05)
ddsResults <- results(dds, contrast = contrastRNR, alpha = 0.05)
## Allows us to see the outcome / verify the correct contrast was made
#mcols(ddsResults, use.name = T)

## Note that if you do not set alpha = 0.05, it's doing the Benjamini-Hochberg adjustment at 0.1. This means that the results of your filtering will likely be different. 

### Set thresholds
padj.cutoff <- 0.05
lfc.cutoff <- 1

## Let's add the row names (gene IDs) back to the results table as a column:
ddsResults$geneID <- row.names(ddsResults)
#names(ddsResults)

## I see a full range of adjusted p-values at this stage.
#hist(ddsResults$padj, col = "lightgray")
#abline(v = 0.05, col = "red", lwd = 2, lty = 2)

## I also see a full range of LFC values at this stage.
#hist(ddsResults$log2FoldChange, col = "lightgray")
#abline(v = c(lfc.cutoff, -lfc.cutoff), col = "blue", lwd = 2, lty = 2)

## P-value filtering only:
sigDGE <- subset(ddsResults, padj < padj.cutoff)
hist(sigDGE$log2FoldChange, col = "lightgray", breaks = 50)
abline(v = c(lfc.cutoff, -lfc.cutoff), col = "blue", lwd = 2, lty = 2)

## How many of these are DEGs?
nrow(sigDGE)
## How many up?
nrow(subset(sigDGE, log2FoldChange > 0))
## How many down?
nrow(subset(sigDGE, log2FoldChange < 0))

## Summary statistics of the LFC for the p-value filtered data. Make sure to do it on the absolute value of the LFC, or you will get the wrong mean.
summary(abs(sigDGE$log2FoldChange))

## Filter on both the p-value & an LFC of the absolute value of the minimal cutoff:
sigDGE <- subset(ddsResults, padj < padj.cutoff & abs(log2FoldChange) > lfc.cutoff)
# hist(sigDGE$padj, col = "lightgray", breaks = 50)
# abline(v = padj.cutoff, col = "blue", lwd = 2, lty = 2)

## How many of these are DEGs?
nrow(sigDGE)
## How many up?
nrow(subset(sigDGE, log2FoldChange > lfc.cutoff))
## How many down?
nrow(subset(sigDGE, log2FoldChange < -lfc.cutoff))

## Filter based on the LFC cutoff only:
# sigDGE <- subset(ddsResults, abs(log2FoldChange) > lfc.cutoff)
# hist(sigDGE$padj, col = "lightgray", breaks = 50)
# abline(v = padj.cutoff, col = "blue", lwd = 2, lty = 2)

# ## How many of these are DEGs?
# nrow(sigDGE)
# ## How many up?
# nrow(subset(sigDGE, padj > padj.cutoff))
# ## How many down?
# nrow(subset(sigDGE, padj < padj.cutoff))
# ## This verifies that number.
```


#### Results Visualization: Volcano Plot, Heatmap

Sort log fold change results by adjusted p-value
```{r}
foldchange <- foldchange[order(foldchange$padj),]
head(foldchange, 3)
```

Plotting the Normalized Counts for our Top 3 Most DE Genes:
```{r}
#Compare the normalized counts between our two groups
par(mfrow=c(1,3))

plotCounts(dds, gene="Caust.v2_010158", intgroup="phenotype")
plotCounts(dds, gene="Caust.v2_001174", intgroup="phenotype")
plotCounts(dds, gene="Caust.v2_020198", intgroup="phenotype")
```

#### Volcano Plot
```{r}
# Make a basic volcano plot
with(foldchange, plot(log2FoldChange, -log10(pvalue), pch=20, cex = 0.75, main="Volcano Plot for LFC = 1.5", xlim=c(-3,3), col ="gray", ylim=c(0,25)))

# Add colored points: blue if padj<0.01, orange if log2FC > 1.5 and padj<0.05)
with(subset(foldchange, padj<.05), points(log2FoldChange, -log10(pvalue), pch=20, col="blue"))
with(subset(foldchange, padj<.05 & abs(log2FoldChange)>1.5), points(log2FoldChange, -log10(pvalue), pch=20, col="orange"))

abline(v=c(-2, 2), col = "black", lty = 2)
abline(h=-log10(0.01), col = "black", lty = 2)

```

#### Top 20 Genes
Not currently working.
```{r, include = FALSE}

# library(tidyverse)
# ## Order results by padj values
# top20_sigOE_genes <- res_tableOE_tb %>% 
#         arrange(padj) %>% 	#Arrange rows by padj values
#         pull(gene) %>% 		#Extract character vector of ordered genes
#         head(n=20) 		#Extract the first 20 genes
# 
# top20_sigOE_genes <- as.data.frame(top20_sigOE_genes)
# 
# ## normalized counts for top 20 significant genes
# top20_sigOE_norm <- as.data.frame(normalized_counts) %>% filter(gene %in% top20_sigOE_genes)
# 
# # Gathering the columns to have normalized counts to a single column
# gathered_top20_sigOE <- top20_sigOE_norm %>% 
#   gather(colnames(top20_sigOE_norm)[2:9], key = "samplename", value = "normalized_counts")
# 
# ## check the column header in the "gathered" data frame
# View(gathered_top20_sigOE)
# 
# # The inner_join() will merge 2 data frames with respect to the "samplename" column, i.e. a column with the same column name in both data frames.
# gathered_top20_sigOE <- inner_join(mov10_meta, gathered_top20_sigOE)
# 
# ## plot using ggplot2
# ggplot(gathered_top20_sigOE) +
#         geom_point(aes(x = gene, y = normalized_counts, color = sampletype)) +
#         scale_y_log10() +
#         xlab("Genes") +
#         ylab("log10 Normalized Counts") +
#         ggtitle("Top 20 Significant DE Genes") +
#         theme_bw() +
# 	theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
# 	theme(plot.title = element_text(hjust = 0.5))
```

#### Heatmap
Also not working yet
```{r. include = FALSE}
### Extract normalized expression for significant genes from the OE and control samples (4:9), and set the gene column (1) to row names
# norm_OEsig <- normalized_counts[,c(1,4:9)] %>% 
#               filter(gene %in% sigOE$gene) %>% 
# 	            data.frame() %>%
# 	            column_to_rownames(var = "gene")
# 
# normalized_counts <- as.data.frame(normalized_counts)
# is.data.frame(sigOE)
```

